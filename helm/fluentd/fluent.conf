    # This file collects and filters all Kubernetes container logs. Should rarely need to modify it.
    # Do not directly collect fluentd's own logs to avoid infinite loops.
    <label @FLUENT_LOG>
      <match fluent.**>
        @type null
      </match>
    </label>

    <source>
      @type tail
      path {{ required "A valid .Values.Fluentd.FilePathsCommaSeparated entry required!" .Values.Fluentd.FilePathsCommaSeparated }} 
      tag azuremigrate.* # it Adds tag azuremigrate.filename on log, you can use for routing in source directive.
      read_from_head true
      <parse>
        @type multi_format
        # Read logs in JSON format for Kubernetes v1.18-
        <pattern>
          format json
          time_format "%Y-%m-%dT%H:%M:%S.%NZ"
          keep_time_key true
        </pattern>
        <pattern>
          format  regexp
          expression /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/
          time_format '%Y-%m-%dT%H:%M:%S.%N%:z'
          keep_time_key true
        </pattern>
      </parse>
    </source> 

    <filter azuremigrate.**>
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field true # this removes the log field if successfully parsed as JSON
      emit_invalid_record_to_error true # In case of unparsable log lines or CRI logs. Keep fluentd's error log clean
      <parse>
        @type multi_format
          <pattern>
            time_key "Timestamp"
            time_format '%Y-%m-%dT%H:%M:%S.%N%:z'
            keep_time_key true
            format json
          </pattern>
          <pattern>
            time_key "time"
            time_format '%Y-%m-%dT%H:%M:%S.%N%:z'
            keep_time_key true
            format none
          </pattern>
        </parse>        
    </filter>

    <filter azuremigrate.**>
        @type kubernetes_metadata # this requires the tag to be in format *.logfilename.
    </filter>
    
    # Flatten fields nested within the 'kubernetes' and 'system data' field and remove unnecessary fields
    <filter azuremigrate.**>
      @type record_transformer
      enable_ruby   
      <record>    
        Timestamp ${ if record.has_key?('time'); then record ["time"]; else record ["Timestamp"]; end}
        ContainerName ${record["kubernetes"]["container_name"]}
        NamespaceName ${record["kubernetes"]["namespace_name"]}    
        PodName ${record["kubernetes"]["pod_name"]}
        Node ${record["kubernetes"]["host"]}
        MasterUrl ${record["kubernetes"]["master_url"]}
        CreatedBy ${ if record.has_key?('systemData'); then record ["systemData"]["createdBy"]; else "Unknown"; end}
        CreatedByType ${ if record.has_key?('systemData'); then record ["systemData"]["createdByType"]; else "Unknown"; end}
        CreatedAt ${ if record.has_key?('systemData'); then record ["systemData"]["createdAt"]; else "Unknown"; end}
        ResourceUID ${ if record.has_key?('systemData'); then record ["systemData"]["resourceUID"]; else "Unknown"; end}
      </record>
      # The logtag field is used in CRI to support multi-line logs. It is usually noise, so remove by default.
      # https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/kubelet-cri-logging.md
      remove_keys docker,kubernetes,stream,logtag,systemData,time
    </filter>

    <filter azuremigrate.**>
      @type record_transformer
      enable_ruby true
      <record>
        ServiceEventName ${ if record.has_key?('ServiceEventName'); then record ["ServiceEventName"]; else record["ServiceEventName"] = "NonTelemetryEvent"; end}
      </record>
    </filter>

    <match azuremigrate.**>
      @type rewrite_tag_filter
      <rule>
        key     ServiceEventName
        pattern ^(.+)$
        tag     azuremigrate$1
      </rule>
    </match>

    <match azuremigrateTelemetryEvent>
      @type rewrite_tag_filter
      <rule>
        key     LogLevel
        pattern ^(.+)$
        tag     TelemetryEvent
      </rule>
    </match>

    <match azuremigrateMetricEvent>
      @type rewrite_tag_filter
      <rule>
        key     LogLevel
        pattern ^(.+)$
        tag     MetricEvent
      </rule>
    </match>

    <match azuremigrateNonTelemetryEvent>
      @type rewrite_tag_filter
      <rule>
        key     LogLevel
        pattern ^(.+)$
        tag     azuremigrateNonTelemetryEvent.$1 
      </rule>
    </match>

    <match azuremigrateNonTelemetryEvent.ERROR>
      @type rewrite_tag_filter
      <rule>
        key     LogLevel
        pattern ^(.+)$
        tag     ErrorEvent
      </rule>
    </match>

    <match azuremigrateNonTelemetryEvent.INFO>
      @type rewrite_tag_filter
      <rule>
        key     LogLevel
        pattern ^(.+)$
        tag     DiagnosticEvent
      </rule>
    </match>

    <match azuremigrateNonTelemetryEvent.**>
      @type rewrite_tag_filter
      <rule>
        key     LogLevel
        pattern ^(.+)$
        tag     ErrorEvent
      </rule>
    </match>

     # Send events to MDSD
    <match **>
      @type mdsd
      @log_level info
      djsonsocket /var/run/mdsd/default_djson.socket
      # Full path to mdsd dynamic json socket file
      acktimeoutms 5000
       # max time in milliseconds to wait for mdsd acknowledge response. If 0, no wait.
      convert_hash_to_json true
      # Convert hash value to valid json. The => are replaced with :     
      # fluentd tag patterns whose match will be used as mdsd source name
      num_threads 1
      buffer_chunk_limit 1000k
      buffer_type file
      buffer_path /var/log/td-agent/buffer/{{ required "A valid .Values.Prefix entry required!" .Values.Prefix }}*.buffer
      buffer_queue_limit 128
      flush_interval 10s
      retry_limit 3
      retry_wait 10s
    </match>

# Catch all unprocessed data and output it, for debugging purposes
 # <match **>
 #  type stdout
 #</match>